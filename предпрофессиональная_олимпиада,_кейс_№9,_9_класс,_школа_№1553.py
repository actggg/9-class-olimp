# -*- coding: utf-8 -*-
"""Предпрофессиональная олимпиада, Кейс №9, 9 класс, Школа №1553

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cTzicrTFg8zRB7cEBGy3nwDjBVZfkKEQ

Блок импорта библиотек
"""

#Импорт библиотек
import pandas as pd
import nltk
import re
import pickle
import matplotlib.pyplot as plt
import numpy as np

from nltk.corpus import stopwords

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn import svm
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report

from keras import models
from keras import layers
from keras.optimizers import Adam
from keras import optimizers

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

"""#Основной код

"""

data_from_internet = pd.read_csv('database.csv', engine='python')

nltk.download("stopwords") #Удаление стоп-слов
nltk.download('punkt') #Деление текста на список предложений
nltk.download('wordnet') #Проведение лемматизации
nltk.download('omw-1.4')

def lemmatize_text(in_text):
    out_text = re.sub("[^a-zA-Z]"," ",in_text) #Удаление неалфавитных символов  
    #text = nltk.word_tokenize(text,language = "english") #Токенизация слов
    out_text = [lemmatize.lemmatize(word) for word in out_text] #Лемматизация слов  
    out_text = "".join(out_text) #Соединение слов
    return out_text

new_text = []
lemmatize = nltk.WordNetLemmatizer()
for i in data_from_internet["content"]:
  new_text.append(lemmatize_text(i))

tfidf_vectorizer = TfidfVectorizer(stop_words="english", min_df=0.01, max_df=0.9  ) 
tfidf_vectorizer = tfidf_vectorizer.fit(new_text)
values = tfidf_vectorizer.transform(new_text) #Преобразование текста
values.shape

#Сохранение векторизатора TFIDF в файл
pkl_filename_tfidf = "Pickle_RL_Model_tfidf.pkl"  

with open(pkl_filename_tfidf, 'wb') as file:  
    pickle.dump(tfidf_vectorizer, file)

X = values.toarray()
y = data_from_internet["class"].values
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42) #Разделение выборки на тестовую и обучающую

nb = GaussianNB()
nb = nb.fit(x_train, y_train)
predicted_bayes = nb.predict(x_test)
print(metrics.classification_report(predicted_bayes, y_test))
nb.score(x_test,y_test)

with open(pkl_filename_tfidf, 'rb') as file:  
    pickled_tfidf_vectorizer = pickle.load(file)

#Сохранение модели в файл
pkl_filename_nb = "Pickle_RL_Model_nb.pkl"  

with open(pkl_filename_nb, 'wb') as file:  
    pickle.dump(nb, file)

#Загрузка из файла 
with open(pkl_filename_nb, 'rb') as file:  
    pickled_nb_model = pickle.load(file)

list_text =[]
for num_f in range(3):
  file_name = f"f6_0{str(num_f)}.txt"
  with open(file_name) as f:
    t = f.read()
    list_text.append(t)

texts_for_predict = list_text + ["Education is about learning skills and knowledge. It also means helping people to learn how to do things and support them to think about what they learn. It's also important for educators to teach ways to find and use information. Education needs research to find out how to make it better", 
            "Education may help and guide individuals from one class to other. Educated individuals and groups can do things like, help less educated people and encourage them to get educated."]

test_text = [lemmatize_text(text) for text in texts_for_predict] #Предподготовка

X_test_vector = pickled_tfidf_vectorizer.transform(test_text) #Векторизация нового текста
X_test_vector.shape

with open(pkl_filename_nb, 'rb') as file:  
    pickled_nb_model = pickle.load(file)

#Вероятность
Y_test_pred=pickled_nb_model.predict(X_test_vector.toarray())
for result in Y_test_pred:
  print(f"Вероятность для всех классов: {result}")

#Задаем порог 0.5 для классификации текста
Y_test_pred_int=(Y_test_pred>=0.5).astype("int")

for result in Y_test_pred_int:
  print(f"Результаты классификации для каждого класса (порог 0.5): {result}")

"""## Логистическая регрессия (LogisticRegression)"""

logreg = LogisticRegression()
logreg = logreg.fit(x_train, y_train)
predicted_logreg = logreg.predict(x_test)
print(metrics.classification_report(predicted_logreg, y_test))
logreg.score(x_test,y_test)

#Сохранение модели в файл
pkl_filename_logreg = "Pickle_RL_Model_logreg.pkl"  

with open(pkl_filename_logreg, 'wb') as file:  
    pickle.dump(logreg, file)

with open(pkl_filename_logreg, 'rb') as file:  
    pickled_logreg_model = pickle.load(file)

#Вероятность
Y_test_pred=pickled_logreg_model.predict(X_test_vector.toarray())
for result in Y_test_pred:
  print(f"Вероятность для всех классов: {result}")

#Задаем порог 0.5 для классификации текста
Y_test_pred_int=(Y_test_pred>=0.5).astype("int")

for result in Y_test_pred_int:
  print(f"Результаты классификации для каждого класса (порог 0.5): {result}")

"""### Метод опорных векторов (SVM)"""

metodsvm = svm.SVC()
metodsvm = metodsvm.fit(x_train, y_train)
predicted_svm = metodsvm.predict(x_test)
print(metrics.classification_report(predicted_svm, y_test))
metodsvm.score(x_test, y_test)

#Сохранение модели в файл
pkl_filename_svm = "Pickle_RL_Model_svm.pkl"  

with open(pkl_filename_svm, 'wb') as file:  
    pickle.dump(metodsvm, file)

with open(pkl_filename_svm, 'rb') as file:  
    pickled_svm_model = pickle.load(file)

#Вероятность
Y_test_pred=pickled_svm_model.predict(X_test_vector.toarray())
for result in Y_test_pred:
  print(f"Определился класс: {result}")

"""То, что не вошло в программу"""

#Адаптивный бустинг
modelClf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=20), n_estimators=200, random_state=42)

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.33, random_state = 42)

modelClf_fit = modelClf.fit(X_train, y_train)
modelClf.score(X_valid, y_valid)

#Сохранение модели в файл
pkl_filename_Clf_Ada = "Pickle_RL_Model_Clf_Ada.pkl"  

with open(pkl_filename_Clf_Ada, 'wb') as file:  
    pickle.dump(modelClf, file)

with open(pkl_filename_Clf_Ada, 'rb') as file:  
    pickled_Clf_Ada_model = pickle.load(file)

#Градиентный бустинг
modelClf = GradientBoostingClassifier(max_depth=20, n_estimators=150,random_state=12, learning_rate=0.005)

X_train, X_valid, y_train, y_valid = train_test_split(X, y,
                                                      test_size=0.3, random_state=12)

modelClf.fit(X_train, y_train)
modelClf.score(X_valid, y_valid)

#Сохранение модели в файл
pkl_filename_Clf_Grad = "Pickle_RL_Model_Clf_Grad.pkl"  

with open(pkl_filename_Clf_Grad, 'wb') as file:  
    pickle.dump(modelClf, file)

with open(pkl_filename_Clf_Grad, 'rb') as file:  
    pickled_Clf_Grad_model = pickle.load(file)

x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)

model = models.Sequential()

voc_len = 26678

model.add(layers.Dense(64,activation='relu'))# ,input_shape=(voc_len,)
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(1, activation = 'sigmoid'))

#model.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])
model.compile(optimizer=Adam(learning_rate=1e-3),loss='binary_crossentropy',metrics=['accuracy'])

history=model.fit(np.array(x_train), np.array(y_train), epochs=20, verbose=0, batch_size=512,validation_data=(x_test,y_test))

history_dict = history.history
history_dict.keys()
# dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])

#Построение графика потери на этапах проверки и обучения
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(history_dict['accuracy'])+1)

#Построение графика потери на этапах проверки и обучения
loss_values = history_dict['loss']
val_loss_values = history_dict['val_loss']
epochs = range(1, len(history_dict['accuracy'])+1)

plt.plot(epochs, loss_values, 'bo', label = 'Потери на этапе обучения')
plt.plot(epochs, val_loss_values, 'b', label = 'Потери на этапе проверки')
plt.title('Потери на этапах обучения и проверки')
plt.xlabel('Эпохи')
plt.ylabel('Потери')
plt.legend()
plt.show()

#Построение графика точности на этапах обучения и проверки
acc_values = history_dict['accuracy']
val_acc_values = history_dict['val_accuracy']
plt.plot(epochs, acc_values, 'bo', label = 'Точность на этапе обучения')
plt.plot(epochs, val_acc_values, 'b', label = 'Точность на этапе проверки')
plt.title('Точность на этапах обучения и проверки')
plt.xlabel('Эпохи')
plt.ylabel('Точность')
plt.legend()
plt.show()

Y_pred=model.predict(x_test)
#Задаем порог 0,5 для классификации текста
Y_pred=(Y_pred>=0.5).astype("int")
print(classification_report(y_test,Y_pred))
print(confusion_matrix(y_test,Y_pred))